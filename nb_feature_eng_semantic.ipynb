{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from gensim import corpora, models\n",
    "from textblob import TextBlob\n",
    "import operator\n",
    "import mord\n",
    "\n",
    "from src import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "db_file = './data/db_master_clean.pickle'\n",
    "df = pd.read_pickle(db_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prepare feature matrix \"X\" and target \"y\" for Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_filter = '12'\n",
    "#target_filter = '134'\n",
    "\n",
    "X, y = ml_preprocessing(df, filter=target_filter)\n",
    "print X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def X_lsi_additional(X_lsi, y):\n",
    "\n",
    "    print X_lsi.shape, y.shape\n",
    "    mask = [len(list_elem) == num_topics for list_elem in X_lsi]\n",
    "    y = y[mask]\n",
    "    X_lsi = X_lsi[mask]\n",
    "    print X_lsi.shape, y.shape\n",
    "\n",
    "    _X_lsi = [elem for list_elem in X_lsi for elem in list_elem]\n",
    "    _X2_lsi = [_X_lsi[i:i+num_topics] for i in range(0, len(_X_lsi), num_topics)]\n",
    "    X_lsi = np.array(_X2_lsi)\n",
    "    print X_lsi.shape, y.shape\n",
    "    \n",
    "    return X_lsi, y\n",
    "\n",
    "    \n",
    "def create_features_semantic(X):   \n",
    "    \n",
    "    #def print_feature_report(X_tfidf, min_df, max_df): \n",
    "    #    print \">>> feature report:\"\n",
    "    #    X_length, _ = Xtfidf[:,0].shape\n",
    "    #    print \"%d texts\" % X_length\n",
    "    #    print \"required minimal word occurence of %.1f\" % min_df\n",
    "    #    print \"required maximal word occurence of %.1f\" % max_df\n",
    "    #    print \"global number of different words in texts:\", X_tfidf.count_nonzero()\n",
    "    \n",
    "    # filter words\n",
    "    #cv_dict = count_vectorizer.vocabulary_\n",
    "    #no_samples, no_words = X_counts.todense().shape\n",
    "    #x = {}\n",
    "    #for key in cv_dict:\n",
    "    #    x[key] = X_counts.todense().sum(axis=0)[0,cv_dict[key]]\n",
    "    #sorted_x = sorted(x.items(), key=operator.itemgetter(1))\n",
    "    #print '20 most frequently occurring words:'\n",
    "    #filtered_words = [elem for elem, number in sorted_x[:-21:-1]]\n",
    "    \n",
    "    \n",
    "    X_tok = [tokenizeText(cleanText(elem)) for elem in X]\n",
    "    #X_tokk = [[tok for tok in tok_list if tok in filtered_words] for tok_list in X_tok]\n",
    "    X_tokk = X_tok\n",
    "    dictionary = corpora.Dictionary(X_tokk)\n",
    "    print dictionary\n",
    "\n",
    "    ###### Tfidf\n",
    "    corpus = [dictionary.doc2bow(elem) for elem in X_tokk]\n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "\n",
    "    ###### Latent Semantic Analysis\n",
    "    lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=num_topics)\n",
    "    corpus_lsi = lsi[corpus_tfidf]\n",
    "    print '\\n>>> Lsi topics:'\n",
    "    _lsi = [elem for elem in lsi.print_topics(num_topics)]\n",
    "    for elem in _lsi:\n",
    "        print elem\n",
    "    \n",
    "    print '\\n>>> First few Lsi features:'\n",
    "    for i, doc in enumerate(corpus_lsi): # both bow->tfidf and tfidf->lsi transformations are actually executed here, on the fly\n",
    "        print(doc)\n",
    "        if i == 5:\n",
    "            break\n",
    "\n",
    "    ###### Turn into feature matrix\n",
    "    _X_lsi = [[topic_correlation for (_,topic_correlation) in doc] for doc in corpus_lsi]\n",
    "    X_lsi = np.array(_X_lsi)#.reshape(len(lsi_features_clean)/num_topics,num_topics)\n",
    "    \n",
    "    #X_lsi, y = X_lsi_additional(X_lsi, y)\n",
    "    \n",
    "    return X_lsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print '\\n--------------------------------'\n",
    "print '--------------------------------\\n'\n",
    "\n",
    "if 'semantic' in feature_type:\n",
    "    %time Xlsi = create_features_semantic(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
